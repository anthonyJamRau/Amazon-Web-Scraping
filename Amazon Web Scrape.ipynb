{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48c0d172",
   "metadata": {},
   "source": [
    "\n",
    "Where I got the idea for this project:https://www.youtube.com/watch?v=HiOtQMcI5wg&t=1614s\n",
    "\n",
    "Beautiful Soup documentation:  https://www.crummy.com/software/BeautifulSoup/bs4/doc/#id12 \n",
    "\n",
    "Some basic headers for requests module found at: # http://httpbin.org/get "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a99f8ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required python packages\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import smtplib\n",
    "import pandas as pd\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import random\n",
    "from itertools import cycle\n",
    "from lxml.html import fromstring\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.backends.backend_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1284941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proxies():\n",
    "    # Method to scrape a list of free proxy IP addresses for aid in webscraping Amazon pages\n",
    "    # returns: \n",
    "    #         - Set, a list of the current available proxies, updated on the website every 10 minutes (1/1/2023)\n",
    "    url = 'https://free-proxy-list.net/'\n",
    "    response = requests.get(url)\n",
    "    parser = fromstring(response.text)\n",
    "    proxies = set()\n",
    "    for i in parser.xpath('//tbody/tr')[:10]:\n",
    "        if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
    "            proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
    "            proxies.add(proxy)\n",
    "    return proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bea5844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_winning_proxy(URL,headers,proxies,prev_winner):\n",
    "    # Method that tries to connect to a URL with the proxies returned from the get_proxies() method. Utilizes recursion to keep retrying this method until successful.\n",
    "    #  \n",
    "    # parameters:\n",
    "    #            - URL : String, The web page address we are scraping\n",
    "    #            - headers : Dictionary, the values to pass with the requests method for authenticication purposes\n",
    "    #            - proxies : Set, the list of proxies returned from get_proxies()\n",
    "    #            - prev_winner: String, the previous IP adderess that we successfully connected to the web page with\n",
    "    # returns:\n",
    "    #            - Soup : a beautiful soup object representing the indexed raw html code of our target web page\n",
    "    #            - proxy: the new IP that we were able to connect to the web page with\n",
    "    if prev_winner != 0:\n",
    "        try:\n",
    "            response = requests.get(URL,headers=headers,proxies={\"http\": prev_winner, \"https\": prev_winner})\n",
    "            \n",
    "            # check for the static value that is returned from the web if the request is blocked\n",
    "            if \"Something went wrong!\" in bs(response.text,\"html.parser\").text:\n",
    "                print(\"no luck\")\n",
    "            else:\n",
    "                Soup = bs(response.text, \"html.parser\")\n",
    "                print(\"lucky\")\n",
    "                return [Soup,prev_winner]\n",
    "        except: \n",
    "            print(\"Skipping. Connnection error\")\n",
    "    \n",
    "    proxy_pool = cycle(proxies)\n",
    "\n",
    "    for i in range(1,11):\n",
    "        proxy = next(proxy_pool)\n",
    "        try:\n",
    "            response = requests.get(URL,headers=headers,proxies={\"http\": proxy, \"https\": proxy})\n",
    "            \n",
    "            if \"Something went wrong!\" in bs(response.text,\"html.parser\").text:\n",
    "                print(\"no luck\")\n",
    "                if i == 10:\n",
    "                    print(\"retrying in 60s\")\n",
    "                    time.sleep(60)\n",
    "                    proxies = get_proxies()\n",
    "\n",
    "                    #recursive element\n",
    "                    get_winning_proxy(URL,headers,proxies,prev_winner)\n",
    "                next\n",
    "            else:\n",
    "                Soup = bs(response.text, \"html.parser\")\n",
    "                print(\"lucky\")\n",
    "                return [Soup,proxy]\n",
    "        except:\n",
    "            print(\"Skipping. Connnection error\")\n",
    "            if i == 10:\n",
    "                print(\"retrying in 60s\")\n",
    "                time.sleep(60)\n",
    "                proxies = get_proxies()\n",
    "\n",
    "                #recursive element\n",
    "                get_winning_proxy(URL,headers,proxies,prev_winner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84147242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getnextpage(Soup,headers,prev_winner):\n",
    "    # Method that seaches the Amazon page for the pagination data in order to grab the next page of results\n",
    "    # parameters:\n",
    "    #            - Soup : a beautiful soup object representing the indexed raw html code of our target web page\n",
    "    #            - headers : Dictionary, the values to pass with the requests method for authenticication purposes\n",
    "    #            - prev_winner: String, the previous IP adderess that we successfully connected to the web page with\n",
    "    # # returns:\n",
    "    #            - Soup : a beautiful soup object representing the indexed raw html code of our target web page\n",
    "    #            - winner: the new IP that we were able to connect to the web page with\n",
    "\n",
    "    proxies = get_proxies()\n",
    "    if(Soup.find('span',{'class': 's-pagination-strip'})):\n",
    "        paginate = Soup.find('span',{'class': 's-pagination-strip'})\n",
    "        if(paginate.find('a',{'class':'s-pagination-item s-pagination-next s-pagination-button s-pagination-separator'})):\n",
    "            url = 'https://www.amazon.com'+str(paginate.find('a',{'class':'s-pagination-item s-pagination-next s-pagination-button s-pagination-separator'})['href'])\n",
    "            res = get_winning_proxy(url,headers,proxies,prev_winner)\n",
    "            print(res[1])\n",
    "            Soup = res[0]\n",
    "            winner = res[1]\n",
    "        else:\n",
    "            return \"End of Results\"\n",
    "    else:\n",
    "        return \"End of Results\"\n",
    "    return [Soup,winner]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fc53f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    # Executes the entire data scraping of an Amazon results page. Since I used this to price and rating data for computer hardware, \n",
    "    # some of the strings to search for in the html are hardcoded and will need to be changed for different use cases. How\n",
    "    # you structure the results will be important when changing how the dataframes are named and initialized.\n",
    "    # This code can be modified by other programmers with a knowledge of beautiful soup and requests modules to scrape results\n",
    "    # from another website that gives pages of results using link based pagination. \n",
    "    # Will output a single pdf with graphs showing the price history of all the products in the results page.\n",
    "    # Each time this is ran, another day of data is going to be added to the charts. All of the history is stored as \n",
    "    # CSV that the delta data is added to.\n",
    "\n",
    "    # ENTER THE PAGE OF AMAZON RESULTS THAT YOU WANT TO SCRAPE\n",
    "    URL = ''\n",
    "\n",
    "    headers = {\n",
    "    'authority': 'www.amazon.com',\n",
    "    'pragma': 'no-cache',\n",
    "    'cache-control': 'no-cache',\n",
    "    'dnt': '1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (X11; CrOS x86_64 8172.45.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.64 Safari/537.36',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'sec-fetch-site': 'none',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "    }\n",
    "\n",
    "    # First time connecting to the URL\n",
    "    proxies = get_proxies()\n",
    "    prev_winner = 0\n",
    "    res = get_winning_proxy(URL,headers,proxies,prev_winner)\n",
    "    Soup = res[0]\n",
    "    winner = res[1]\n",
    "\n",
    "    # initialize DataFrame to store the results\n",
    "    df = pd.DataFrame(columns = ['Title','Price','Date','Rating','Sample Size'])\n",
    "    tSoup = Soup\n",
    "    todayDate = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # loop through the amazon pages and append the results to the DataFrames\n",
    "    while tSoup != \"End of Results\":\n",
    "\n",
    "        try:\n",
    "            x = Soup.find_all('div',{'class':'a-section a-spacing-small a-spacing-top-small'})\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        for i in range(len(x)):\n",
    "            if(x[i].find(class_='a-price-whole')):\n",
    "                price_to_append = x[i].find(class_='a-price-whole').text.strip()\n",
    "            else:\n",
    "                price_to_append = \"N/A\"\n",
    "\n",
    "            if(x[i].find(class_ = \"a-size-medium a-color-base a-text-normal\")):\n",
    "                title_to_append = x[i].find(class_ = \"a-size-medium a-color-base a-text-normal\").text.strip()\n",
    "            else:\n",
    "                title_to_append = \"N/A\"\n",
    "\n",
    "            if(x[i].find('div',{'class':'a-row a-size-small'})):\n",
    "                if(x[i].find('div',{'class':'a-row a-size-small'}).find('span',{'aria-label':True})):\n",
    "                    ratings = x[i].find('div',{'class':'a-row a-size-small'}).find_all('span',{'aria-label':True})\n",
    "                    stars = ratings[0].text.strip()\n",
    "                    samples = ratings[1].text.strip()\n",
    "            else:\n",
    "                stars = \"N/A\"\n",
    "                samples = \"N/A\"\n",
    "\n",
    "\n",
    "            df.loc[len(df.index)] = [title_to_append,price_to_append,todayDate,stars,samples]\n",
    "        time.sleep(5)     \n",
    "        res = getnextpage(Soup,headers,winner)\n",
    "        tSoup = res[0]\n",
    "        winner = res[1]\n",
    "\n",
    "        Soup = tSoup\n",
    "\n",
    "    # Initial cleaning of the data\n",
    "    step1 = df.where(df['Title'] != 'N/A').where(df['Price'] != 'N/A').dropna().drop_duplicates()\n",
    "    step2 = step1.loc[step1['Title'].str.contains(\"3070\")]\n",
    "\n",
    "    # Initialize separate DataFrames for each brand of GPU \n",
    "    MSI_df = step2.loc[step2['Title'].str.contains(\"msi\",case=False)]\n",
    "    Gigabyte_df = step2.loc[step2['Title'].str.contains(\"gigabyte\",case=False)]\n",
    "    Asus_df = step2.loc[step2['Title'].str.contains(\"Asus\",case=False)]\n",
    "    Evga_df = step2.loc[step2['Title'].str.contains(\"Evga\",case=False)]\n",
    "    Nvidia_df = step2.loc[step2['Title'].str.contains(\"Nvidia\",case=False)]\n",
    "    Zotac_df = step2.loc[step2['Title'].str.contains(\"Zotac\",case=False)]\n",
    "\n",
    "    # Will need to have these CSV files created in the same directory\n",
    "    archive = ['MSI_history.csv','Gigabyte_history.csv','Nvidia_history.csv','Zotac_history.csv','Asus_history.csv','Evga_history.csv']\n",
    "    delta = [MSI_df,Gigabyte_df,Nvidia_df,Zotac_df,Asus_df,Evga_df]\n",
    "\n",
    "    pdf = matplotlib.backends.backend_pdf.PdfPages(\"AmazonOutput.pdf\")\n",
    "\n",
    "    # Further cleaning of the data, storing the new data, and creating the charts\n",
    "    counter = 0\n",
    "    for name in archive:\n",
    "\n",
    "        df_v1 = delta[counter]\n",
    "        df_v1 = df_v1.where(df_v1['Rating'] != 'N/A').dropna()\n",
    "\n",
    "        # clean title\n",
    "        df_v1['Title'] = df_v1['Title'].str.slice(0,110)\n",
    "\n",
    "        # clean price\n",
    "        try:\n",
    "            df_v1[\"Price\"] = df_v1[\"Price\"].astype(float) \n",
    "        except:\n",
    "            df_v1[\"Price\"] = df_v1[\"Price\"].astype(str)\n",
    "            df_v1[\"Price\"] = df_v1[\"Price\"].str.replace(',','')\n",
    "            df_v1[\"Price\"] = df_v1[\"Price\"].str.replace('.','')\n",
    "            df_v1[\"Price\"] = df_v1[\"Price\"].astype(int)  \n",
    "\n",
    "        #clean rating\n",
    "        df_v1[\"Rating\"] = df_v1[\"Rating\"].str.slice(0,3)\n",
    "        df_v1[\"Rating\"] = df_v1[\"Rating\"].astype(float)\n",
    "\n",
    "        #clean sample\n",
    "        df_v1[\"Sample Size\"] = df_v1[\"Sample Size\"].astype(str)\n",
    "        df_v1[\"Sample Size\"] = df_v1[\"Sample Size\"].str.replace('(','')\n",
    "        df_v1[\"Sample Size\"] = df_v1[\"Sample Size\"].str.replace(')','')\n",
    "        df_v1[\"Sample Size\"] = df_v1[\"Sample Size\"].astype(float)\n",
    "\n",
    "        tHistory = pd.read_csv(name)\n",
    "        df_v1 = pd.concat([tHistory,df_v1])\n",
    "        df_v1['Date'] = pd.to_datetime(df_v1['Date'],unit='ns')\n",
    "        df_v1.to_csv(name,index=False)\n",
    "        df_v1.sort_values(by=[\"Date\"])\n",
    "\n",
    "        names = df_v1[\"Title\"].unique()\n",
    "        df_list = list()\n",
    "\n",
    "        # plot the data using matplotlib and place it on the pdf\n",
    "        for i in range(len(names)):\n",
    "            df = df_v1.loc[df_v1['Title'] == names[i]]\n",
    "            df_list.append(df)\n",
    "\n",
    "        figure = plt.figure(figsize = (11,13))\n",
    "        ax = figure.add_subplot(111)\n",
    "        ax.set_position([0.10,0.60,0.8,0.3])\n",
    "        formats = ['p-','r-', 'y-', 'g-', 'b-','o-', 'r--', 'g--', 'y--', 'b--', 'o--','p--','p-','r-', 'y-', 'g-', 'b-','o-', 'r--', 'g--', 'y--', 'b--', 'o--','p--']\n",
    "        formats1 = ['yo', 'r^', 'go', 'y*', 'bo', 'ro', 'g*', 'y^', 'b^', 'yo', 'g^','yo', 'r^', 'go', 'y*', 'bo', 'ro', 'g*', 'y^', 'b^', 'yo', 'g^']\n",
    "        for i in range(len(df_list)):\n",
    "            if(len(df_list[i].index)>1):\n",
    "                ax.plot(df_list[i]['Date'],df_list[i]['Price'],formats[i],label = str(df_list[i]['Title'].iloc[0]),figure=figure)\n",
    "            else:\n",
    "                ax.plot(df_list[i]['Date'],df_list[i]['Price'],formats1[i],label = str(df_list[i]['Title'].iloc[0]),figure=figure)\n",
    "        plt.title(name,figure=figure)\n",
    "        legend = ax.legend(bbox_to_anchor =(0.5,-1.3), loc='lower center')\n",
    "        plt.show()\n",
    "        pdf.savefig(figure)\n",
    "        counter = counter + 1\n",
    "    pdf.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "ebabb01f54876d71cf1d6c99f4a2910ef5401db97980552731c8c161cd3d5ea5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
